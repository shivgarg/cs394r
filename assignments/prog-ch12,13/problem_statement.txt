

In this programming assignment, we will implement (1) the true online Sarsa(λ) algorithm described on page 307 of the textbook, (2) REINFORCE: Monte-Carlo Gradient Control (episodic) algorithm with baseline (page 330).

The environments that are considered for this assignment are again from OpenAI gym. You need to implement two function and two classes. Similar to the previous assignment, you have to keep the function and class interface intact since it will be auto-graded later. Please download the skeleton code, and follow the instructions below.

(1) Implement true online Sarsa(λ) algorithm using tile coding (`sarsa.py`)

In this part, we implement the true online Sarsa(λ) algorithm as described on page 307 of the textbook. Open `sarsa.py` and implement `SarsaLambda` method. Tile coding should be used as a feature extractor x(s,a). To do so, you should fill in the parts indicated as 'TODO' in the `StateActionFeatureVectorWithTile` class.

Here are a few tips & specific details you need to be careful of when implementing tile coding:

- Each tiling should cover the entire space completely so that every point is included by the same number of tilings.
- In order to achieve that, # tiles in each dimension should be: ceil[(high-low)/tile_width] + 1.
- Each tiling should start from (low - tiling_index / # tilings * tile width) where the tiling index starts from 0. With the starting offset, you can easily find out the corresponding tile given state for each tiling

(2) REINFORCE with baseline using neural networks (`reinforce.py`)

Now, we will implement the REINFORCE algorithm with baseline. To do so, we model the policy and the value function for a baseline with neural networks. We again recommend using a deep learning framework either Tensorflow (v1.14.0) or Pytorch (v1.2.0) for this assignment. If you are not familiar with either of these, please see some tutorials before working on this part. Here are the resources for TensorFlow and PyTorch.

You should fill in the parts indicated as TODO in the 'REINFORCE' function and the methods of 'VApproximationWithNN' and 'PiApproximationWithNN' class. The 'REINFORCE' function should return the Monte-Carlo return at time 0 (i.e. G0) of all the episodes that were executed during the learning. Note that these statistics can represent the progress of learning.

To guarantee convergence, here are few specifics to follow:

- Use AdamOptimizer with beta1=0.9, beta2=0.999 and learning rate <=3∗10−4
- Use two hidden layers with 32 nodes each and ReLU activation function. The final layer for the policy should be a softmax over the n actions, and that for the value function should have no activation function.

(3) Test your code

You can test your implementation with provided `test_sarsa.py` and `test_reinforce.py` scripts. Please run the scripts and check whether it passes the assertion, or observe the plot generated by the script.
